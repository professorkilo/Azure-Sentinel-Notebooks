{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b4a88a",
   "metadata": {
    "tags": [
     "tag"
    ]
   },
   "source": [
    "# Retroactive Threat Intelligence Matching\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook correlates Threat Intelligence Indicators, from the ThreatIntelIndicators table, with log data from multiple sources over a configurable lookback period, aggregates matches by TI indicator, and saves results to a managed table for further analysis.  Future runs will reference matches already in the result set to avoid generating duplicate alerts each time this notebook is run.\n",
    "\n",
    "### How to Run Notebook\n",
    "\n",
    "Reference the general [Sentinel Notebook Readme](./README.md) for guidance on installing and running notebooks.  \n",
    "\n",
    "For this job specifically there is job yaml file included.  Action required by users on that job yaml:\n",
    "- **StartTime**: What day and time the job should start running.\n",
    "- **EndTime**: What day and time the job should stop running.\n",
    "- **JobName(Optional)**: If you decide to change the jobname, prefix the name with 'TI-Retroactive-Hunting'. \n",
    "\n",
    "### Key Features:\n",
    "- **Multiple log source support**\n",
    "- **Flexible matching modes**: current (TI valid now), loose (ignore validity)\n",
    "- **Configurable log sources**: Enable/disable different log types as needed.\n",
    "- **Adjustable lookback period**: Configure how long back it should look for matches.\n",
    "\n",
    "### Currently Supported Log Sources:\n",
    "- **SigninLogs**: Standard user sign-in activities.  This includes the following tables: SigninLogs, AADNonInteractiveUserSignInLogs, AADServicePrincipalSignInLogs, AADManagedIdentitySignInLogs.\n",
    "- **Syslogs**: General table for logging system and security events.\n",
    "- **CommonSecurityLogs**: Table for collecting events in the Common Event format from different security sources.\n",
    "\n",
    "### Required Customer Input:\n",
    "- **WORKSPACE_NAME**: Customer Log Analytics workspace name.  This will be used for retrieving indicator and log data, as well as for outputing match results.  If 'None' is provided then the notebook will look for the first log analytics workspace that is not the Sentinel generated 'default' workspace.\n",
    "- **LOOKBACK_DAYS**: 14-365.  Lookback time period for logs matching.  Default 30.\n",
    "- **MATCH_MODE**: Which ThreatIntelIndicators to match against which logs: current (TI valid now), loose (ignore validity).  Default \"current\".\n",
    "- Enabled the log sources that you would like to match against under the `LOG SOURCE TOGGLES - SUPPORTED` section.\n",
    "\n",
    "### Output Schema:\n",
    "Results are aggregated by TI indicator with match counts and event references for detailed analysis.  The RetroThreatMatchResults_SPRK_CL output table will be generated on the provided Log Analytics workspace.\n",
    "\n",
    "| Column Name | Type |Description |\n",
    "|-------------|------|------------|\n",
    "|MatchId | string |  Unique identifier for the match result record (reference to the original ThreatIntelIndicators Id) | \n",
    "|JobId | string | Identifier for the retroactive matching job execution.  This is a random uuid created by the notebook. |\n",
    "| JobStartTime          | datetime         | Timestamp when the retro-matching job started. |\n",
    "| JobEndTime            | datetime         | Timestamp when the retro-matching job completed. |\n",
    "| MatchType             | string           | Type of match (e.g., \"IoC\", \"Observable\", \"CVE\", \"TTP\"/\"MITRE-Technique\"). |\n",
    "| ObservableType        | string           | Subtype of the match (e.g., \"IP\", \"Domain\", \"URL\", \"SHA256\", \"x509\", \"JA3\").| \n",
    "| ObservableValue       | string           | Observable value (IoC value == Observable value).  Domain, IP, URL, etc. |\n",
    "| TIReferenceId         | string           | Reference to the Threat Intelligence record (e.g., internal IoC ID or STIX ID). |\n",
    "| TIValue               | string           | Actual IoC or observable value that was matched (e.g., \"malicious.com\", name of TTP, etc.). |\n",
    "| MatchCount            | int              | Number of events or records in the environment that matched this TI object. |\n",
    "| EventReferences       | dynamic          | Array of matched events with format `[{\"Table\":\"SigninLogs\",\"RecordId\":\"abc123\"}, ...]`. |\n",
    "| TTPs                  | dynamic          | Array of MITRE techniques (e.g., `[\"T1059\", \"T1071.001\"]`) associated with the matched TI. |\n",
    "| ThreatActors          | dynamic          | Array of threat actor names tied to the matched TI object. |\n",
    "| EnrichmentContext     | dynamic          | Optional dictionary of enrichment tags (e.g., industry, country, malware family, confidence score). |\n",
    "| TenantId              | string           | Identifier of the customer environment (multi-tenant scenarios). |\n",
    "| TimeGenerated | datetime | Timestamp of record creation in this table. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a22d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# PARAMETERS AND LOG SOURCE CONFIGURATION\n",
    "# ===============================================================================\n",
    "\n",
    "# Workspace and Data Configuration\n",
    "WORKSPACE_NAME = None  # log analytics workspace required to be set by customer; or leave as None to auto-detect the first non-default workspace\n",
    "LOOKBACK_DAYS = 30  # Days to look back for logs (default: 1 month)\n",
    "\n",
    "# Matching Mode Configuration - Default \"current\"\n",
    "# - \"current\": TI indicator must be valid at the current time\n",
    "# - \"loose\": Ignore TI validity windows entirely\n",
    "MATCH_MODE = \"current\"\n",
    "\n",
    "# ===============================================================================\n",
    "# LOG SOURCE TOGGLES - SUPPORTED\n",
    "# ===============================================================================\n",
    "ENABLE_SIGNIN_LOGS = True\n",
    "ENABLE_SYS_LOGS = True\n",
    "ENABLE_COMMON_SECURITY_LOGS = True\n",
    "ENABLE_NON_INTERACTIVE_SIGNIN_LOGS = True\n",
    "ENABLE_SERVICE_PRINCIPAL_SIGNIN_LOGS = True\n",
    "ENABLE_MANAGED_IDENTITY_SIGNIN_LOGS = True\n",
    "\n",
    "# ===============================================================================\n",
    "# LOG SOURCE TOGGLES - WORK IN PROGRESS\n",
    "# ===============================================================================\n",
    "ENABLE_WINDOWS_EVENT_LOGS = False\n",
    "ENABLE_SECURITY_EVENT_LOGS = False\n",
    "ENABLE_SECURITY_IOT_RAW_EVENT_LOGS = False\n",
    "ENABLE_OFFICE_LOGS = False\n",
    "ENABLE_DNS_LOGS = False\n",
    "ENABLE_EVENT_LOGS = False\n",
    "ENABLE_W3CIIS_LOGS = False\n",
    "ENABLE_AUDIT_LOGS = False\n",
    "ENABLE_USER_RISK_EVENTS = False\n",
    "\n",
    "\n",
    "# ===============================================================================\n",
    "# DEBUG AND TESTING CONFIGS\n",
    "# ===============================================================================\n",
    "SHOW_DEBUG_LOGS = False\n",
    "REDUCED_DEBUG_LOGS = True  # If True, only show summary debug logs at the end of the script, and counts\n",
    "SHOW_STATS = False\n",
    "USE_TEST_DATA_LOGS = False\n",
    "USE_TEST_DATA_THREAT_INTEL = False\n",
    "\n",
    "# Performance switches\n",
    "AUTO_TUNE_SHUFFLE_PARTITIONS = False  # If True, estimate shuffle partitions before joins\n",
    "TARGET_PARTITION_SIZE_BYTES = 256 * 1024 * 1024  # 256MB\n",
    "\n",
    "# Table Names\n",
    "THREAT_INTEL_TABLE = \"ThreatIntelIndicators\"\n",
    "THREAT_INTEL_OBJECTS_TABLE = \"ThreatIntelObjects\"\n",
    "RESULTS_TABLE = \"RetroThreatMatchResults_SPRK_CL\"\n",
    "\n",
    "# Version number\n",
    "VERSION = \"1.0.2\"\n",
    "\n",
    "# ===============================================================================\n",
    "# LOG CONFIG IMPORTS AND SOURCE MAP\n",
    "# ===============================================================================\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import re\n",
    "\n",
    "# Define structs for certain logs configs\n",
    "SECURITY_IOT_RAW_EVENT_LOGS_STRUCT = StructType([\n",
    "    StructField(\"LocalAddress\", StringType(), True),\n",
    "    StructField(\"RemoteAddress\", StringType(), True)\n",
    "])\n",
    "WINDOWS_EVENT_LOGS_STRUCT = StructType([\n",
    "    StructField(\"TargetUserName\", StringType(), True),\n",
    "    StructField(\"FileHash\", StringType(), True),\n",
    "    StructField(\"SourceAddress\", StringType(), True),\n",
    "    StructField(\"DestAddress\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Regex patterns\n",
    "URL_REGEX_PAT = r\"(https?://(?:[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+(?::[^@\\s]+)?@)?(?!-)(?:[a-zA-Z0-9](?:[a-zA-Z0-9-]*[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,}(?::\\d{1,5})?(?:(?:/[-a-zA-Z0-9._~:/?#\\[\\]@!$&*+=,;%]*[a-zA-Z0-9_/])?|(?:\\?[-a-zA-Z0-9._~:/?#\\[\\]@!$&*+=,;%=]*[a-zA-Z0-9_=&])|(?:#[-a-zA-Z0-9._~:/?#\\[\\]@!$&*+=,;%=]*[a-zA-Z0-9_]))?)\"\n",
    "DOMAIN_REGEX_PAT = r\"(?:^|\\s|[^\\w.])((?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z]{2,}(?![@])|(?<=@)(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z]{2,})(?=$|\\s|[^\\w]|[.]))\"\n",
    "IPV4_PAT = r\"(?<![.\\d])(?:(?!0+\\.0+\\.0+\\.0+)(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3})(?![\\w.])\"\n",
    "IPV6_PAT = r\"(?<![:\\d])(?:(?:[0-9A-Fa-f]{1,4}:){7}[0-9A-Fa-f]{1,4}|(?:(?:[0-9A-Fa-f]{1,4}:){1,7}:|(?:(?:[0-9A-Fa-f]{1,4}:){0,6}[0-9A-Fa-f]{1,4})?::(?:(?:[0-9A-Fa-f]{1,4}:){0,6}[0-9A-Fa-f]{1,4})?))(?:%[0-9A-Za-z]+)?(?![:.\\w])\"\n",
    "IP_REGEX_PAT = \"(?:\" + IPV4_PAT + \"|\" + IPV6_PAT + \")\"\n",
    "\n",
    "# Additional regex patterns\n",
    "EVENT_KV_REGEX = r'<Data Name=\\\"(\\w+)\\\">{?([^<]*?)}?</Data>'\n",
    "EVENT_KV_KEYS = ['Hashes']\n",
    "\n",
    "# Common compiled regexs\n",
    "IP_REGEX = re.compile(IP_REGEX_PAT)\n",
    "URL_REGEX = re.compile(URL_REGEX_PAT)\n",
    "DOMAIN_REGEX = re.compile(DOMAIN_REGEX_PAT)\n",
    "\n",
    "# Log Sources\n",
    "LOG_SOURCES = {\n",
    "    \"SigninLogs\": {\n",
    "        \"table_name\": \"SigninLogs\",\n",
    "        \"id_field\": \"Id\",\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"enabled\": ENABLE_SIGNIN_LOGS,\n",
    "        \"description\": \"Standard user sign-in logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IPAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"ManagedIdentitySigninLogs\": {\n",
    "        \"table_name\": \"AADManagedIdentitySignInLogs\",\n",
    "        \"id_field\": \"Id\",\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"enabled\": ENABLE_MANAGED_IDENTITY_SIGNIN_LOGS,\n",
    "        \"description\": \"Managed identity sign-in logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IPAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"AuditLogs\": {\n",
    "        \"table_name\": \"AuditLogs\",\n",
    "        \"id_field\": \"Id\",\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"enabled\": ENABLE_AUDIT_LOGS,\n",
    "        \"description\": \"Azure AD audit logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IPAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"NonInteractiveUserSignInLogs\": {\n",
    "        \"table_name\": \"AADNonInteractiveUserSignInLogs\",\n",
    "        \"id_field\": \"Id\",\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"enabled\": ENABLE_NON_INTERACTIVE_SIGNIN_LOGS,\n",
    "        \"description\": \"Non-interactive user sign-ins\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IPAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\" ]}\n",
    "        ]\n",
    "    },\n",
    "    \"ServicePrincipalSignInLogs\": {\n",
    "        \"table_name\": \"AADServicePrincipalSignInLogs\",\n",
    "        \"id_field\": \"Id\",\n",
    "        \"tenant_field\": \"TenantId\", \n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"enabled\": ENABLE_SERVICE_PRINCIPAL_SIGNIN_LOGS,\n",
    "        \"description\": \"Service principal sign-in logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IPAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"UserRiskEvents\": {\n",
    "        \"table_name\": \"AADUserRiskEvents\",\n",
    "        \"id_field\": \"Id\",\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\", \n",
    "        \"enabled\": ENABLE_USER_RISK_EVENTS,\n",
    "        \"description\": \"User risk events\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IPAddress\",  \"supported_indicator_types\": [\"ipv4-addr:value\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"Syslog\": {\n",
    "        \"table_name\": \"Syslog\",\n",
    "        \"id_field\": None,\n",
    "        \"tenant_field\": \"TenantId\",    \n",
    "        \"time_field\": \"TimeGenerated\", \n",
    "        \"enabled\": ENABLE_SYS_LOGS,\n",
    "        \"description\": \"System logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"SyslogMessage\", \"log_field_array_regex\": DOMAIN_REGEX, \"supported_indicator_types\": [\"domain-name:value\"]},\n",
    "            {\"log_field\": \"SyslogMessage\", \"log_field_array_regex\": URL_REGEX, \"supported_indicator_types\": [\"url:value\"]},\n",
    "            {\"log_field\": \"SyslogMessage\", \"log_field_array_regex\": IP_REGEX, \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "            {\"log_field\": \"HostIP\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"WindowsEventLogs\": {\n",
    "        \"table_name\": \"WindowsEvent\", \n",
    "        \"json_field\": \"EventData\",\n",
    "        \"json_struct\": WINDOWS_EVENT_LOGS_STRUCT,\n",
    "        \"id_field\": \"EventId\",\n",
    "        \"tenant_field\": \"TenantId\",     \n",
    "        \"time_field\": \"TimeGenerated\", \n",
    "        \"enabled\": ENABLE_WINDOWS_EVENT_LOGS,\n",
    "        \"description\": \"Windows event logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"EventData.TargetUserName\", \"supported_indicator_types\": [\"email-addr:value\"]},\n",
    "            {\"log_field\": \"EventData.FileHash\", \"supported_indicator_types\": [\"file:hashes.MD5\", \"file:hashes.SHA-1\", \"file:hashes.SHA-256\", \"file:ctime\"]},\n",
    "            {\"log_field\": \"EventData.SourceAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "            {\"log_field\": \"EventData.DestAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "        ]\n",
    "    },\n",
    "    \"SecurityEventLogs\": {\n",
    "        \"table_name\": \"SecurityEvent\", \n",
    "        \"id_field\": \"EventOriginId\",\n",
    "        \"tenant_field\": \"TenantId\",      \n",
    "        \"time_field\": \"TimeGenerated\",   \n",
    "        \"enabled\": ENABLE_SECURITY_EVENT_LOGS,\n",
    "        \"description\": \"Security event logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IpAddress\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "            {\"log_field\": \"TargetUserName\", \"supported_indicator_types\": [\"email-addr:value\"]},\n",
    "            {\"log_field\": \"FileHash\", \"supported_indicator_types\": [\"file:hashes.MD5\", \"file:hashes.SHA-1\", \"file:hashes.SHA-256\"]},\n",
    "        ]\n",
    "    },\n",
    "    \"SecurityIoTRawEventLogs\": {\n",
    "        \"table_name\": \"SecurityIoTRawEvent\", \n",
    "        \"json_field\": \"EventDetails\",\n",
    "        \"json_struct\": SECURITY_IOT_RAW_EVENT_LOGS_STRUCT,\n",
    "        \"id_field\": \"IoTRawEventId\",                \n",
    "        \"tenant_field\": None,      \n",
    "        \"time_field\": \"TimeGenerated\",   \n",
    "        \"enabled\": ENABLE_SECURITY_IOT_RAW_EVENT_LOGS,\n",
    "        \"description\": \"Security IoT raw event logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"nested_data.LocalAddress\",  \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "            {\"log_field\": \"nested_data.RemoteAddress\",  \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "        ]\n",
    "    },\n",
    "    \"OfficeLogs\": {\n",
    "        \"table_name\": \"OfficeActivity\",  \n",
    "        \"id_field\": \"OfficeId\",\n",
    "        \"tenant_field\": \"TenantId\",      \n",
    "        \"time_field\": \"TimeGenerated\",   \n",
    "        \"enabled\": ENABLE_OFFICE_LOGS,\n",
    "        \"description\": \"Office logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"ClientIP\",   \"log_field_array_regex\": \"\\\\[?(::ffff:)?((?:\\\\d{1,3}\\\\.){3}\\\\d{1,3}|[a-fA-F0-9:]+)(?:%\\\\d+)?\\\\]?\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "            {\"log_field\": \"UserId\",   \"supported_indicator_types\": [\"email-addr:value\"]},\n",
    "        ]\n",
    "    },\n",
    "    \"DnsLogs\": {\n",
    "        \"table_name\": \"DnsEvents\", \n",
    "        \"id_field\": \"EventId\",                   \n",
    "        \"tenant_field\": \"TenantId\",         \n",
    "        \"time_field\": \"TimeGenerated\",      \n",
    "        \"enabled\": ENABLE_DNS_LOGS,\n",
    "        \"description\": \"DNS logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"IPAddresses\", \"log_separator\": \",\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "            {\"log_field\": \"Name\",  \"supported_indicator_types\": [\"domain-name:value\"]},\n",
    "        ]\n",
    "    },\n",
    "    \"CommonSecurityLogs\": {\n",
    "        \"table_name\": \"CommonSecurityLog\",\n",
    "        \"id_field\": None,\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"enabled\": ENABLE_COMMON_SECURITY_LOGS,\n",
    "        \"description\": \"Common security logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"RequestURL\", \"log_field_value_regex\": DOMAIN_REGEX_PAT, \"supported_indicator_types\": [\"domain-name:value\"]},\n",
    "            {\"log_field\": \"AdditionalExtensions\", \"log_field_value_regex\": DOMAIN_REGEX_PAT, \"supported_indicator_types\": [\"domain-name:value\"]},\n",
    "            {\"log_field\": \"RequestURL\", \"supported_indicator_types\": [\"url:value\"]},\n",
    "            {\"log_field\": \"AdditionalExtensions\", \"log_field_value_regex\": URL_REGEX_PAT, \"supported_indicator_types\": [\"url:value\"]},\n",
    "            {\"log_field\": \"FileHash\", \"supported_indicator_types\": [\"file:hashes.MD5\", \"file:hashes.SHA-1\", \"file:hashes.SHA-256\"]},\n",
    "            {\"log_field\": \"SourceIP\",\"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "            {\"log_field\": \"DestinationIP\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]}\n",
    "        ]\n",
    "    },\n",
    "    \"EventLogs\": {\n",
    "        \"table_name\": \"Event\",\n",
    "        \"id_field\": \"EventId\",\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"nested_regex_field\": \"EventData\",\n",
    "        \"nested_regex_pattern\": EVENT_KV_REGEX,\n",
    "        \"nested_regex_keys\": EVENT_KV_KEYS,\n",
    "        \"enabled\": ENABLE_EVENT_LOGS,\n",
    "        \"description\": \"Event logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"Compute\", \"supported_indicator_types\": [\"domain-name:value\"]},\n",
    "            {\"log_field\": \"nested_data.Hashes\",  \"log_field_array_regex\": \"(?<=:)\\\\s*([^,]*)\", \"supported_indicator_types\": [\"file:hashes.MD5\", \"file:hashes.SHA-1\", \"file:hashes.SHA-256\"]},\n",
    "        ]\n",
    "    },\n",
    "    \"W3CIISLogs\": {\n",
    "        \"table_name\": \"W3CIISLog\",\n",
    "        \"id_field\": \"todo\",\n",
    "        \"tenant_field\": \"TenantId\",\n",
    "        \"time_field\": \"TimeGenerated\",\n",
    "        \"enabled\": ENABLE_W3CIIS_LOGS,\n",
    "        \"description\":  \"W3C IIS logs\",\n",
    "        \"join_conditions\": [\n",
    "            {\"log_field\": \"cIP\", \"supported_indicator_types\": [\"ipv4-addr:value\", \"ipv6-addr:value\", \"network-traffic:src_ref.value\", \"network-traffic:dst_ref.value\"]},\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ===============================================================================\n",
    "# PARAMETER VALIDATION\n",
    "# ===============================================================================\n",
    "if LOOKBACK_DAYS <= 0:\n",
    "    raise ValueError(\"LOOKBACK_DAYS must be positive\")\n",
    "\n",
    "if MATCH_MODE not in [\"strict\", \"current\", \"loose\"]:\n",
    "    raise ValueError(\"MATCH_MODE must be one of: strict, current, loose\")\n",
    "\n",
    "if not RESULTS_TABLE or not isinstance(RESULTS_TABLE, str) or RESULTS_TABLE.strip() == \"\":\n",
    "    raise ValueError(\"RESULTS_TABLE must be a non-empty string before saving results.\")\n",
    "if not RESULTS_TABLE.endswith('_SPRK_CL'):\n",
    "    RESULTS_TABLE = f\"{RESULTS_TABLE}_SPRK_CL\"\n",
    "\n",
    "enabled_sources = [source for source, config in LOG_SOURCES.items() if config[\"enabled\"]]\n",
    "if not enabled_sources and not USE_TEST_DATA_LOGS:\n",
    "    raise ValueError(\"At least one log source must be enabled OR USE_TEST_DATA_LOGS must be True\")\n",
    "\n",
    "for source_name, config in LOG_SOURCES.items():\n",
    "    for idx, join_condition in enumerate(config.get(\"join_conditions\", [])):\n",
    "        has_value_regex = \"log_field_value_regex\" in join_condition\n",
    "        has_array_regex = \"log_field_array_regex\" in join_condition\n",
    "        if has_value_regex and has_array_regex:\n",
    "            raise ValueError(\n",
    "                f\"Configuration error in {source_name} join_condition[{idx}]: Cannot specify both 'log_field_value_regex' and 'log_field_array_regex'.\"\n",
    "            )\n",
    "\n",
    "print(\"Notebook version:\", VERSION)\n",
    "print(f\"Configuration loaded: {WORKSPACE_NAME}, {LOOKBACK_DAYS} days lookback, '{MATCH_MODE}' matching mode\")\n",
    "if enabled_sources:\n",
    "    print(f\"Enabled log sources: {', '.join(enabled_sources)}\")\n",
    "else:\n",
    "    print(\"No real log sources enabled - using test data only for fast execution\")\n",
    "\n",
    "# Collect supported indicator types from ENABLED sources only\n",
    "supported_observable_keys = sorted({\n",
    "    it\n",
    "    for _, cfg in LOG_SOURCES.items()\n",
    "    if cfg.get(\"enabled\")\n",
    "    for jc in cfg.get(\"join_conditions\", [])\n",
    "    for it in jc.get(\"supported_indicator_types\", [])\n",
    "})\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    print(f\"Collected {len(supported_observable_keys)} unique indicator types from ENABLED log sources:\")\n",
    "    for key in supported_observable_keys:\n",
    "        print(f\"  • {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb6aa7",
   "metadata": {},
   "source": [
    "## Imports, Sentinel Provider, and Spark Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99cab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ===============================================================================\n",
    "import json, uuid\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    broadcast, expr, lit, current_timestamp, col, array, struct, when,\n",
    "    count as spark_count, row_number, first, collect_list, flatten, size,\n",
    "    get_json_object, from_json, to_json, explode, regexp_extract, split, trim, lower, map_from_arrays,\n",
    "    concat_ws, array_distinct, array_union, coalesce, udf, sum, count_distinct, collect_set\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StringType, ArrayType, StructType, StructField, TimestampType\n",
    ")\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from sentinel_lake.providers import MicrosoftSentinelProvider\n",
    "\n",
    "# Start time\n",
    "start = time.time()\n",
    "\n",
    "# Spark conf levers (AQE, skew, local shuffle, parquet, thresholds)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", str(2 * TARGET_PARTITION_SIZE_BYTES))\n",
    "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(150 * 1024 * 1024))\n",
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "spark.conf.set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "spark.conf.set(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", str(TARGET_PARTITION_SIZE_BYTES))\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", str(TARGET_PARTITION_SIZE_BYTES))\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", str(TARGET_PARTITION_SIZE_BYTES))\n",
    "\n",
    "# Provider init + load base TI tables\n",
    "data_provider = MicrosoftSentinelProvider(spark)\n",
    "print(\"✓ Microsoft Sentinel data provider initialized successfully\")\n",
    "\n",
    "# Ensure enrichment mapping variable is defined (may be populated later)\n",
    "indicator_actor_by_indicator = None\n",
    "\n",
    "# Job start time for logging and tracking\n",
    "job_start_time = current_timestamp()\n",
    "\n",
    "# Helper: quick sample count\n",
    "def sample_count(df, sample_rate=0.01):\n",
    "    try:\n",
    "        sc = df.sample(fraction=sample_rate, seed=42).count()\n",
    "        return int(sc / sample_rate)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "# Optional: auto-tune shuffle partitions using rough size estimate\n",
    "def maybe_auto_tune_shuffle(df_list):\n",
    "    if not AUTO_TUNE_SHUFFLE_PARTITIONS:\n",
    "        return\n",
    "    try:\n",
    "        MIN_LIMIT = 200\n",
    "        MAX_LIMIT = 10000\n",
    "        def est_parts(df):\n",
    "            samp = df.limit(MAX_LIMIT).rdd.map(lambda r: len(str(r))).mean()\n",
    "            cnt = df.count()\n",
    "            est_bytes = samp * cnt\n",
    "            return max(1, int(est_bytes // TARGET_PARTITION_SIZE_BYTES))\n",
    "        parts = max(est_parts(d) for d in df_list if d is not None)\n",
    "        if SHOW_DEBUG_LOGS: print(f\"Max estimated partitions={parts}, setting limits [{MIN_LIMIT}, {MAX_LIMIT}]\")\n",
    "        parts = max(MIN_LIMIT, min(MAX_LIMIT, parts))\n",
    "        spark.conf.set(\"spark.sql.shuffle.partitions\", str(parts))\n",
    "        if SHOW_DEBUG_LOGS: print(f\"Auto-tuned spark.sql.shuffle.partitions={parts}\")\n",
    "    except Exception as e:\n",
    "        if SHOW_DEBUG_LOGS: print(f\"Auto-tune skipped: {e}\")\n",
    "\n",
    "\n",
    "if WORKSPACE_NAME is None or WORKSPACE_NAME.strip() == \"\":\n",
    "    print(\"No workspace name provided, automatically selecting the first available non-default Log Analytics workspace.\")\n",
    "\n",
    "    databases = data_provider.list_databases()\n",
    "    for db in databases:\n",
    "        if db not in [\"default\"]:\n",
    "            WORKSPACE_NAME = db\n",
    "            print(f\"Auto-selected workspace: {WORKSPACE_NAME}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3eb36",
   "metadata": {},
   "source": [
    "## Threat actor enrichment via ThreatIntelObjects (with fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740693b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# LOAD THREAT INTEL OBJECTS AND BUILD INDICATOR -> THREAT ACTORS\n",
    "# ===============================================================================\n",
    "try:\n",
    "    ti_objects_df = data_provider.read_table(THREAT_INTEL_OBJECTS_TABLE, WORKSPACE_NAME)\n",
    "    ti_objects_df = ti_objects_df.withColumnRenamed(\n",
    "        \"TenantId\", \"TIO_TenantId\"\n",
    "    ).withColumnRenamed(\"TimeGenerated\", \"TIO_TimeGenerated\")\n",
    "\n",
    "    parsed = ti_objects_df.select(\n",
    "        col(\"StixType\").alias(\"ObjectType\"),\n",
    "        get_json_object(col(\"Data\"), \"$.id\").alias(\"ObjectId\"),\n",
    "        get_json_object(col(\"Data\"), \"$.name\").alias(\"name\"),\n",
    "        get_json_object(col(\"Data\"), \"$.source_ref\").alias(\"source_ref\"),\n",
    "        get_json_object(col(\"Data\"), \"$.target_ref\").alias(\"target_ref\"),\n",
    "        get_json_object(col(\"Data\"), \"$.relationship_type\").alias(\"relationship_type\"),\n",
    "        from_json(\n",
    "            get_json_object(col(\"Data\"), \"$.aliases\"), ArrayType(StringType())\n",
    "        ).alias(\"aliases\"),\n",
    "        from_json(\n",
    "            get_json_object(col(\"Data\"), \"$.threat_actor_types\"),\n",
    "            ArrayType(StringType()),\n",
    "        ).alias(\"threat_actor_types\"),\n",
    "        col(\"Data\").alias(\"ObjectData\"),  # keep raw JSON payload here\n",
    "    )\n",
    "\n",
    "    # relationships\n",
    "    relationships = parsed.filter(col(\"ObjectType\") == lit(\"relationship\")).select(\n",
    "        \"source_ref\", \"target_ref\"\n",
    "    )\n",
    "\n",
    "    # attack-patterns and actors (include ObjectData in actors so we can reference it later)\n",
    "    attack_patterns = parsed.filter(col(\"ObjectType\") == lit(\"attack-pattern\")).select(\n",
    "        col(\"ObjectId\").alias(\"AttackPatternRef\"),\n",
    "        col(\"ObjectData\"),\n",
    "    )\n",
    "\n",
    "    actors = parsed.filter(col(\"ObjectType\") == lit(\"threat-actor\")).select(\n",
    "        col(\"ObjectId\").alias(\"ThreatActorId\"),\n",
    "        col(\"name\").alias(\"ThreatActorName\"),\n",
    "        col(\"aliases\").alias(\"ThreatActorAliases\"),\n",
    "        col(\"threat_actor_types\").alias(\"ThreatActorTypes\"),\n",
    "        col(\"ObjectData\"),\n",
    "    )\n",
    "\n",
    "    # attack-pattern <-> threat-actor relationships (both directions)\n",
    "    ap2ta = relationships.filter(\n",
    "        col(\"source_ref\").startswith(\"attack-pattern--\")\n",
    "        & col(\"target_ref\").startswith(\"threat-actor--\")\n",
    "    ).select(\n",
    "        col(\"source_ref\").alias(\"AttackPatternRef\"),\n",
    "        col(\"target_ref\").alias(\"ThreatActorRef\"),\n",
    "    )\n",
    "\n",
    "    ap2ta_rev = relationships.filter(\n",
    "        col(\"target_ref\").startswith(\"attack-pattern--\")\n",
    "        & col(\"source_ref\").startswith(\"threat-actor--\")\n",
    "    ).select(\n",
    "        col(\"target_ref\").alias(\"AttackPatternRef\"),\n",
    "        col(\"source_ref\").alias(\"ThreatActorRef\"),\n",
    "    )\n",
    "\n",
    "    ap2ta_all = ap2ta.union(ap2ta_rev).dropDuplicates()\n",
    "\n",
    "    # parse external_references from attack-pattern ObjectData\n",
    "    ext_ref_item = StructType(\n",
    "        [\n",
    "            StructField(\"external_id\", StringType(), True),\n",
    "            StructField(\"source_name\", StringType(), True),\n",
    "            StructField(\"url\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "    ext_ref_wrapper = StructType(\n",
    "        [StructField(\"external_references\", ArrayType(ext_ref_item), True)]\n",
    "    )\n",
    "\n",
    "    attack_patterns_parsed = attack_patterns.select(\n",
    "        \"AttackPatternRef\",\n",
    "        from_json(col(\"ObjectData\"), ext_ref_wrapper)\n",
    "        .getField(\"external_references\")\n",
    "        .alias(\"external_references\"),\n",
    "        get_json_object(col(\"ObjectData\"), \"$.name\").alias(\"name\"),\n",
    "    )\n",
    "\n",
    "    # empty array literal\n",
    "    empty_str_arr = from_json(lit(\"[]\"), ArrayType(StringType()))\n",
    "\n",
    "    # extract external_id array without explode (optionally filter by source_name == 'mitre-attack' if desired)\n",
    "    ttp_expr = \"array_distinct(filter(transform(external_references, x -> x.external_id), x -> x is not null))\"\n",
    "\n",
    "    # SQL-only extraction from name: match a MITRE token at the start of the (trimmed) name.\n",
    "    # Pattern explanation:\n",
    "    #  - ^(?i)        : start of string, case-insensitive\n",
    "    #  - (T\\d+(?:\\.\\d+)*) : capture T + digits, optional .digits groups (e.g., T1583 or T1583.003)\n",
    "    #  - (?:\\s|:|$)   : ensure token is followed by space, colon, or end-of-string\n",
    "    #\n",
    "    # We use regexp_extract(trim(name), <pattern>, 1) to capture the token, then wrap into an array,\n",
    "    # filter out empty, uppercase, and dedupe with array_distinct.\n",
    "    ttp_from_name_expr = (\n",
    "        \"array_distinct(\"\n",
    "        \"  transform(\"\n",
    "        \"    filter(\"\n",
    "        \"      array(regexp_extract(trim(name), '^(?i)(T\\\\\\\\d+(?:\\\\\\\\.\\\\\\\\d+)*)(?:\\\\\\\\s|:|$)', 1)),\"\n",
    "        \"      x -> x IS NOT NULL AND x <> ''\"\n",
    "        \"    ),\"\n",
    "        \"    x -> upper(x)\"\n",
    "        \"  )\"\n",
    "        \")\"\n",
    "    )\n",
    "\n",
    "    # Final TTPs: prefer external_references when present/non-empty; otherwise extract from name start\n",
    "    ap_ttps = attack_patterns_parsed.select(\n",
    "        \"AttackPatternRef\",\n",
    "        when(\n",
    "            (col(\"external_references\").isNotNull()) & (size(col(\"external_references\")) > 0),\n",
    "            expr(ttp_expr),\n",
    "        )\n",
    "        .otherwise(\n",
    "            when(col(\"name\").isNotNull(), expr(ttp_from_name_expr)).otherwise(empty_str_arr)\n",
    "        )\n",
    "        .alias(\"TTPs\"),\n",
    "    )\n",
    "\n",
    "    # ensure referenced attack-patterns have entries\n",
    "    ap_refs = ap2ta_all.select(\"AttackPatternRef\").distinct()\n",
    "    ap_ttps_all = ap_refs.join(ap_ttps, on=\"AttackPatternRef\", how=\"left\").select(\n",
    "        col(\"AttackPatternRef\"), coalesce(col(\"TTPs\"), empty_str_arr).alias(\"TTPs\")\n",
    "    )\n",
    "\n",
    "    # roll up TTPs to threat actors\n",
    "    actor_ttp = ap2ta_all.join(ap_ttps_all, on=\"AttackPatternRef\", how=\"left\").select(\n",
    "        col(\"ThreatActorRef\"), col(\"TTPs\")\n",
    "    )\n",
    "\n",
    "    actor_ttp_by_actor = (\n",
    "        actor_ttp.groupBy(\"ThreatActorRef\")\n",
    "        .agg(array_distinct(flatten(collect_list(col(\"TTPs\")))).alias(\"TTPs\"))\n",
    "        .select(\n",
    "            col(\"ThreatActorRef\"), coalesce(col(\"TTPs\"), empty_str_arr).alias(\"TTPs\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Extracted {actor_ttp_by_actor.count()} threat actors with TTPs from ThreatIntelObjects\"\n",
    "    )\n",
    "    if SHOW_DEBUG_LOGS:\n",
    "        actor_ttp_by_actor.show(10, truncate=False)\n",
    "\n",
    "    # attach actor TTPs into actors_enriched (use qualified column expressions)\n",
    "    a = actors.alias(\"a\")\n",
    "    t = actor_ttp_by_actor.alias(\"t\")\n",
    "\n",
    "    actors_enriched = a.join(\n",
    "        t, col(\"a.ThreatActorId\") == col(\"t.ThreatActorRef\"), how=\"left\"\n",
    "    ).select(\n",
    "        col(\"a.ThreatActorId\").alias(\"ThreatActorRef\"),\n",
    "        col(\"a.ThreatActorName\"),\n",
    "        col(\"a.ThreatActorAliases\"),\n",
    "        col(\"a.ThreatActorTypes\"),\n",
    "        coalesce(col(\"t.TTPs\"), empty_str_arr).alias(\"TTPs\"),\n",
    "        col(\"a.ObjectData\").alias(\"ObjectData\"),\n",
    "    )\n",
    "\n",
    "    print(f\"Enriched threat actors:\")\n",
    "    if SHOW_DEBUG_LOGS:\n",
    "        actors_enriched.show(10, truncate=True)\n",
    "\n",
    "    # --- indicator <-> threat-actor pairs (both directions) ---\n",
    "    i2a = relationships.filter(\n",
    "        col(\"source_ref\").startswith(\"indicator--\")\n",
    "        & col(\"target_ref\").startswith(\"threat-actor--\")\n",
    "    ).select(\n",
    "        col(\"source_ref\").alias(\"IndicatorRef\"),\n",
    "        col(\"target_ref\").alias(\"ThreatActorRef\"),\n",
    "    )\n",
    "\n",
    "    i2a_rev = relationships.filter(\n",
    "        col(\"target_ref\").startswith(\"indicator--\")\n",
    "        & col(\"source_ref\").startswith(\"threat-actor--\")\n",
    "    ).select(\n",
    "        col(\"target_ref\").alias(\"IndicatorRef\"),\n",
    "        col(\"source_ref\").alias(\"ThreatActorRef\"),\n",
    "    )\n",
    "\n",
    "    i2a_all = i2a.union(i2a_rev).dropDuplicates()\n",
    "\n",
    "    # join indicators -> actors_enriched and collect actor names and TTPs per indicator\n",
    "    enriched = i2a_all.join(\n",
    "        broadcast(actors_enriched),\n",
    "        i2a_all.ThreatActorRef == actors_enriched.ThreatActorRef,\n",
    "        \"left\",\n",
    "    ).select(\n",
    "        i2a_all.IndicatorRef.alias(\"IndicatorId\"),\n",
    "        actors_enriched.ThreatActorName,\n",
    "        actors_enriched.ThreatActorAliases,\n",
    "        actors_enriched.ThreatActorTypes,\n",
    "        actors_enriched.TTPs,\n",
    "    )\n",
    "\n",
    "    # build ThreatActors array (name + aliases)\n",
    "    enriched = (\n",
    "        enriched.withColumn(\n",
    "            \"_name_arr\",\n",
    "            when(\n",
    "                col(\"ThreatActorName\").isNotNull(), array(col(\"ThreatActorName\"))\n",
    "            ).otherwise(empty_str_arr),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"ThreatActors\",\n",
    "            array_distinct(\n",
    "                array_union(\n",
    "                    col(\"_name_arr\"), coalesce(col(\"ThreatActorAliases\"), empty_str_arr)\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        .drop(\"_name_arr\")\n",
    "    )\n",
    "\n",
    "    # aggregate to indicator\n",
    "    indicator_actor_by_indicator = enriched.groupBy(\"IndicatorId\").agg(\n",
    "        array_distinct(\n",
    "            flatten(collect_list(coalesce(col(\"ThreatActors\"), empty_str_arr)))\n",
    "        ).alias(\"ThreatActors\"),\n",
    "        array_distinct(\n",
    "            flatten(collect_list(coalesce(col(\"TTPs\"), empty_str_arr)))\n",
    "        ).alias(\"TTPs\"),\n",
    "    )\n",
    "\n",
    "    # normalize defaults\n",
    "    indicator_actor_by_indicator = indicator_actor_by_indicator.withColumn(\n",
    "        \"ThreatActors\",\n",
    "        when(#\n",
    "            (col(\"ThreatActors\").isNull()) | (size(col(\"ThreatActors\")) == 0),\n",
    "            array(lit(\"Unknown Actor\")),\n",
    "        ).otherwise(col(\"ThreatActors\")),\n",
    "    ).withColumn(\"TTPs\", coalesce(col(\"TTPs\"), empty_str_arr))\n",
    "\n",
    "    indicator_actor_by_indicator.cache()\n",
    "    # 1) explode ThreatActors to one actor per row\n",
    "    exploded_by_actor = indicator_actor_by_indicator.select(\n",
    "        F.col(\"IndicatorId\"),\n",
    "        F.explode(F.col(\"ThreatActors\")).alias(\"ThreatActor\"),\n",
    "        F.col(\"TTPs\")\n",
    "    )\n",
    "\n",
    "    # 2) group by actor and aggregate\n",
    "    actors_rolled_up = exploded_by_actor.groupBy(\"ThreatActor\").agg(\n",
    "        F.collect_set(\"IndicatorId\").alias(\"IndicatorIds\"),\n",
    "        # collect_list -> flatten (array of arrays) -> distinct to dedupe TTPs\n",
    "        F.array_distinct(F.flatten(F.collect_list(F.coalesce(F.col(\"TTPs\"), empty_str_arr)))).alias(\"TTPs\")\n",
    "    )\n",
    "\n",
    "    # Optional: sort and show results\n",
    "    actors_rolled_up = actors_rolled_up.orderBy(\"ThreatActor\")\n",
    "    print(f\"Rolled up to {actors_rolled_up.count()} unique threat actors with indicators and TTPs:\")\n",
    "    if SHOW_DEBUG_LOGS:\n",
    "        actors_rolled_up.show(truncate=True)\n",
    "    print(\"✓ ThreatIntelObjects enrichment ready\")\n",
    "\n",
    "except Exception as e:\n",
    "    indicator_actor_by_indicator = None\n",
    "    print(f\"⚠ ThreatIntelObjects not available or failed to load: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1afcf9",
   "metadata": {},
   "source": [
    "## Load and filter log data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# Helper function for creating composite Id fields\n",
    "# ===============================================================================\n",
    "def get_record_id(id_field):\n",
    "    \"\"\"\n",
    "    Create an ID for log entries.\n",
    "    Returns the actual ID or None if no ID field is available.\n",
    "    \"\"\"\n",
    "    if id_field is None:\n",
    "        # Return null for RecordId when there's no ID field\n",
    "        return lit(None).alias(\"Id\")\n",
    "    else:\n",
    "        # Single ID field\n",
    "        return col(id_field).cast(\"string\").alias(\"Id\")\n",
    "\n",
    "\n",
    "# ===============================================================================\n",
    "# Helper function for extracting values\n",
    "# ===============================================================================\n",
    "def extract_all(pattern, text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "\n",
    "# ===============================================================================\n",
    "# Collect table names in the selected workspace\n",
    "# ===============================================================================\n",
    "tables = data_provider.list_tables(WORKSPACE_NAME)\n",
    "table_names = [t.name for t in tables]\n",
    "\n",
    "# ===============================================================================\n",
    "# LOAD AND FILTER LOG DATA (multiple sources, base behavior)\n",
    "# ===============================================================================\n",
    "def load_log_source(source_name, config, start_date, end_date):\n",
    "    try:\n",
    "        # Store the actual table name for use in EventReferences\n",
    "        actual_table_name = config[\"table_name\"]\n",
    "        if actual_table_name not in table_names:\n",
    "            # If a configured table is not found, silently skip it with a warning\n",
    "            print(f\"Warning: {actual_table_name} not found in workspace {WORKSPACE_NAME}, skipping...\")\n",
    "            return []\n",
    "\n",
    "        # Load the log data. We expect the table to exist at this point\n",
    "        df = data_provider.read_table(config[\"table_name\"], WORKSPACE_NAME)\n",
    "        df = df.filter(\n",
    "            (col(config[\"time_field\"]) <= end_date)\n",
    "            & (col(config[\"time_field\"]) >= start_date)\n",
    "        )\n",
    "        # Parse nested formats\n",
    "        if \"json_struct\" in config and \"json_field\" in config:\n",
    "            df = df.withColumn(\n",
    "                \"nested_data\",\n",
    "                from_json(col(config[\"json_field\"]), config[\"json_struct\"]),\n",
    "            )\n",
    "        elif \"nested_regex_field\" in config and \"nested_regex_pattern\" in config:\n",
    "            df = (\n",
    "                df.withColumn(\n",
    "                    \"nested_data_raw\",\n",
    "                    map_from_arrays(\n",
    "                        expr(\n",
    "                            f\"regexp_extract_all(`{config['nested_regex_field']}`, '{config['nested_regex_pattern']}', 1)\"\n",
    "                        ),\n",
    "                        expr(\n",
    "                            f\"regexp_extract_all(`{config['nested_regex_field']}`, '{config['nested_regex_pattern']}', 2)\"\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "                .withColumn(\n",
    "                    \"nested_data\",\n",
    "                    struct(\n",
    "                        *[\n",
    "                            col(\"nested_data_raw\").getItem(k).alias(k)\n",
    "                            for k in config[\"nested_regex_keys\"]\n",
    "                        ]\n",
    "                    ),\n",
    "                )\n",
    "                .drop(\"nested_data_raw\")\n",
    "            )\n",
    "        dfs = []\n",
    "        for jc in config.get(\"join_conditions\", []):\n",
    "            print(f\"join condition: {jc}\")\n",
    "            log_field = jc.get(\"log_field\")\n",
    "            if \".\" in log_field and not log_field.startswith(\"nested_data.\"):\n",
    "                raise ValueError(\n",
    "                    f\"Configuration error in {source_name}: Nested field '{log_field}' is not supported unless under 'nested_data.*'\"\n",
    "                )\n",
    "            field_col = (\n",
    "                col(log_field)\n",
    "                if \".\" not in log_field or log_field.startswith(\"nested_data.\")\n",
    "                else col(log_field)\n",
    "            )\n",
    "            if \"log_field_value_regex\" in jc:\n",
    "                extracted = regexp_extract(field_col, jc[\"log_field_value_regex\"], 1)\n",
    "                sub = df.select(\n",
    "                    lit(actual_table_name).alias(\"LogSource\"),\n",
    "                    trim(lower(extracted)).alias(\"ObservableValue\"),\n",
    "                    lit(config[\"id_field\"]).alias(\"IdField\"),\n",
    "                    get_record_id(config[\"id_field\"]),\n",
    "                    (\n",
    "                        col(config.get(\"tenant_field\"))\n",
    "                        if config.get(\"tenant_field\")\n",
    "                        else lit(None)\n",
    "                    ).alias(\"TenantId\"),\n",
    "                    col(config[\"time_field\"]).alias(\"TimeGenerated\"),\n",
    "                    lit(log_field).alias(\"LogField\"),\n",
    "                    extracted.alias(\"OriginalValue\")\n",
    "                )\n",
    "            elif \"log_field_array_regex\" in jc:\n",
    "                extract_all_udf = udf(lambda text: extract_all(jc['log_field_array_regex'], text), ArrayType(StringType()))\n",
    "\n",
    "                # Extract all matches directly as an array\n",
    "                temp = df.withColumn(\"_extracted_array\", \n",
    "                    extract_all_udf(field_col)\n",
    "                )\n",
    "\n",
    "                # Explode the array to get individual values\n",
    "                temp = temp.withColumn(\"_extracted_value\", \n",
    "                    explode(col(\"_extracted_array\"))\n",
    "                )\n",
    "\n",
    "                sub = temp.select(\n",
    "                    lit(actual_table_name).alias(\"LogSource\"),\n",
    "                    trim(lower(col(\"_extracted_value\"))).alias(\"ObservableValue\"),\n",
    "                    lit(config[\"id_field\"]).alias(\"IdField\"),\n",
    "                    get_record_id(config[\"id_field\"]),\n",
    "                    (\n",
    "                        col(config.get(\"tenant_field\")).alias(\"TenantId\")\n",
    "                        if config.get(\"tenant_field\")\n",
    "                        else lit(None).alias(\"TenantId\")\n",
    "                    ),\n",
    "                    col(config[\"time_field\"]).alias(\"TimeGenerated\"),\n",
    "                    lit(log_field).alias(\"LogField\"),\n",
    "                    col(\"_extracted_value\").alias(\"OriginalValue\")\n",
    "                ).filter(\n",
    "                    col(\"ObservableValue\").isNotNull() & (col(\"ObservableValue\") != \"\")\n",
    "                )\n",
    "            else:\n",
    "                if \"log_separator\" in jc:\n",
    "                    temp = df.withColumn(\n",
    "                        \"_temp_array\", split(field_col, jc[\"log_separator\"])\n",
    "                    ).withColumn(\"_temp_exploded\", explode(col(\"_temp_array\")))\n",
    "                    sub = temp.select(\n",
    "                        lit(actual_table_name).alias(\"LogSource\"),\n",
    "                        trim(lower(col(\"_temp_exploded\"))).alias(\"ObservableValue\"),\n",
    "                        lit(config[\"id_field\"]).alias(\"IdField\"),\n",
    "                        get_record_id(config[\"id_field\"]),\n",
    "                        (\n",
    "                            col(config.get(\"tenant_field\"))\n",
    "                            if config.get(\"tenant_field\")\n",
    "                            else lit(None)\n",
    "                        ).alias(\"TenantId\"),\n",
    "                        col(config[\"time_field\"]).alias(\"TimeGenerated\"),\n",
    "                        lit(log_field).alias(\"LogField\"),\n",
    "                        col(\"_temp_exploded\").alias(\"OriginalValue\")\n",
    "                    )\n",
    "                else:\n",
    "                    sub = df.select(\n",
    "                        lit(actual_table_name).alias(\"LogSource\"),\n",
    "                        trim(lower(field_col)).alias(\"ObservableValue\"),\n",
    "                        lit(config[\"id_field\"]).alias(\"IdField\"),\n",
    "                        get_record_id(config[\"id_field\"]),\n",
    "                        (\n",
    "                            col(config.get(\"tenant_field\"))\n",
    "                            if config.get(\"tenant_field\")\n",
    "                            else lit(None)\n",
    "                        ).alias(\"TenantId\"),\n",
    "                        col(config[\"time_field\"]).alias(\"TimeGenerated\"),\n",
    "                        lit(log_field).alias(\"LogField\"),\n",
    "                        field_col.alias(\"OriginalValue\")\n",
    "                    )\n",
    "            sub = sub.filter(\n",
    "                col(\"ObservableValue\").isNotNull() & (col(\"ObservableValue\") != \"\")\n",
    "            )\n",
    "            if \"log_filter_field\" in jc and \"log_filter_value\" in jc:\n",
    "                sub = sub.filter(col(jc[\"log_filter_field\"]) == jc[\"log_filter_value\"])\n",
    "            dfs.append(sub)\n",
    "        if not dfs:\n",
    "            return None\n",
    "        out = dfs[0]\n",
    "        for other in dfs[1:]:\n",
    "            out = out.union(other)\n",
    "        out = out.dropDuplicates([\"Id\", \"ObservableValue\"]).repartition(\n",
    "            \"ObservableValue\"\n",
    "        )\n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            print(f\"✓ Filtered {source_name}\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading {source_name}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Calculate date range for filtering (though not currently used in load_log_source)\n",
    "end_date = datetime.now() + timedelta(days=3)\n",
    "start_date = end_date - timedelta(days=LOOKBACK_DAYS)\n",
    "\n",
    "# Load enabled log sources\n",
    "combined_logs_df = None\n",
    "for source_name, config in LOG_SOURCES.items():\n",
    "    if config.get(\"enabled\", False):\n",
    "        try:\n",
    "            source_df = load_log_source(source_name, config, start_date, end_date)\n",
    "            if source_df is not None:\n",
    "                if combined_logs_df is None:\n",
    "                    combined_logs_df = source_df\n",
    "                else:\n",
    "                    combined_logs_df = combined_logs_df.union(source_df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Skipping {source_name} due to error: {e}\")\n",
    "\n",
    "if combined_logs_df is not None:\n",
    "    combined_logs_df = combined_logs_df.cache()\n",
    "    if SHOW_DEBUG_LOGS or REDUCED_DEBUG_LOGS:\n",
    "        combined_logs_count = combined_logs_df.count()\n",
    "        print(f\"✓ Combined log sources loaded and cached. Total records: {combined_logs_count}\")\n",
    "elif not USE_TEST_DATA_LOGS:\n",
    "    raise RuntimeError(\"No log data loaded and USE_TEST_DATA_LOGS is False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d81d08",
   "metadata": {},
   "source": [
    "## Load and split Threat Intelligence Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df8dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# LOAD AND SPLIT THREAT INTELLIGENCE INDICATORS BY OBSERVABLE KEY\n",
    "# ===============================================================================\n",
    "raw_threat_intel_df = data_provider.read_table(THREAT_INTEL_TABLE, WORKSPACE_NAME)\n",
    "print(f\"✓ Loaded threat intelligence table: {THREAT_INTEL_TABLE}\")\n",
    "\n",
    "# Current-time filter (for MATCH_MODE == 'current'); strict handled later at join using log times\n",
    "current_time = current_timestamp()\n",
    "threat_intel_df = raw_threat_intel_df\n",
    "if MATCH_MODE == \"current\":\n",
    "    threat_intel_df = threat_intel_df.filter((col(\"ValidFrom\") <= current_time) & (col(\"ValidUntil\") >= current_time))\n",
    "if MATCH_MODE in [\"current\", \"strict\"]:\n",
    "    threat_intel_df = threat_intel_df.filter(col(\"IsActive\") == True)\n",
    "\n",
    "indicator_dfs = {}\n",
    "for key in supported_observable_keys:\n",
    "    df = threat_intel_df.filter(col('ObservableKey') == key) \n",
    "    df = df.withColumnRenamed(\"TenantId\", \"TI_TenantId\").withColumnRenamed(\"TimeGenerated\", \"TI_TimeGenerated\")\n",
    "    df = df.withColumn(\"ObservableValue\", trim(lower(col(\"ObservableValue\"))))\n",
    "    # Keep needed columns for later\n",
    "    df = df.select( \n",
    "        \"Id\", \"TI_TenantId\", \"TI_TimeGenerated\", \"ObservableKey\", \"ObservableValue\", \"ValidFrom\", \"ValidUntil\", \"Data\", \"Pattern\"\n",
    "    )\n",
    "    # Enrich ThreatActors via ThreatIntelObjects if available; else fallback to Data JSON field\n",
    "    if indicator_actor_by_indicator is not None:\n",
    "        df = df.withColumn(\"IndicatorId\", get_json_object(col(\"Data\"), \"$.id\"))\n",
    "        df = df.join(indicator_actor_by_indicator, df.IndicatorId == indicator_actor_by_indicator.IndicatorId, \"left\").drop(indicator_actor_by_indicator.IndicatorId)\n",
    "    else:\n",
    "        df = df.withColumn(\"ThreatActors\", when(col(\"Data\").isNotNull(), from_json(get_json_object(col(\"Data\"), \"$.threat_actors\"), ArrayType(StringType()))).otherwise(lit(None)))\n",
    "    # Deduplicate: latest per Id\n",
    "    w = Window.partitionBy(\"Id\").orderBy(col(\"TI_TimeGenerated\").desc())\n",
    "    df = df.withColumn(\"row_num\", row_number().over(w)).filter(col(\"row_num\") == 1).drop(\"row_num\")\n",
    "    df = df.repartition(\"ObservableValue\").cache()\n",
    "    indicator_dfs[key] = df\n",
    "print(f\"✓ Prepared indicator splits for {len(indicator_dfs)} observable keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350281d",
   "metadata": {},
   "source": [
    "## Inject test data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test constants\n",
    "TEST_IP = \"142.202.188.59\"\n",
    "TEST_DOMAIN = \"malicious-test.example.com\"\n",
    "TEST_URL = \"http://malicious-test.example.com/payload\"\n",
    "TEST_FILE_HASH = \"abcd1234567890abcd1234567890abcd1234567890abcd1234567890abcd1234\"\n",
    "TEST_TENANT_ID = \"029c55c8-a7ec-418e-b741-de9d24add5fa\"\n",
    "TEST_TIMESTAMP = datetime.strptime(\"2025-07-15T16:29:31.883Z\", \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "\n",
    "def create_test_log_row(source_name=\"SigninLogs\", observable_key=\"ipv4-addr:value\", observable_value=TEST_IP):\n",
    "    return {\n",
    "        'LogSource': source_name, 'ObservableKey': observable_key, 'ObservableValue': observable_value,\n",
    "        'Id': str(uuid.uuid4()), 'TenantId': TEST_TENANT_ID, 'TimeGenerated': TEST_TIMESTAMP,\n",
    "    }\n",
    "\n",
    "def create_test_ti_row(observable_key=\"ipv4-addr:value\", observable_value=TEST_IP):\n",
    "    any_df = next((df for df in indicator_dfs.values() if df is not None), None)\n",
    "    if any_df is None: return None\n",
    "    test_row = Row(**{f.name: None for f in any_df.schema.fields}).asDict()\n",
    "    indicator_id = f\"indicator--test-{uuid.uuid4()}\"\n",
    "    test_data = {\n",
    "        'pattern': f\"[{observable_key} = '{observable_value}']\",\n",
    "        'pattern_type': 'stix',\n",
    "        'valid_from': '2025-07-15T16:25:04.2001568Z',\n",
    "        'name': f\"Test IOC - {observable_value}\",\n",
    "        'description': 'Test indicator',\n",
    "        'indicator_types': ['WatchList'],\n",
    "        'valid_until': '2025-07-31T21:03:26.8933330Z',\n",
    "        'confidence': 75,\n",
    "        'type': 'indicator',\n",
    "        'id': indicator_id,\n",
    "    }\n",
    "    test_row.update({\n",
    "        'TI_TenantId': TEST_TENANT_ID, 'TI_TimeGenerated': TEST_TIMESTAMP, 'WorkspaceId': TEST_TENANT_ID,\n",
    "        'AzureTenantId': '536279f6-15cc-45f2-be2d-61e352b51eef', 'Id': f\"TEST---{indicator_id}\",\n",
    "        'SourceSystem': 'Test Data Generator', 'LastUpdateMethod': 'TestDataInjection', 'IsDeleted': False,\n",
    "        'AdditionalFields': json.dumps({'TLPLevel': 'Green'}), 'Data': json.dumps(test_data), 'IsActive': True,\n",
    "        'ValidUntil': TEST_TIMESTAMP, 'ValidFrom': TEST_TIMESTAMP, 'Created': TEST_TIMESTAMP, 'Modified': TEST_TIMESTAMP,\n",
    "        'Confidence': test_data['confidence'], 'Pattern': test_data['pattern'],\n",
    "        'ObservableKey': observable_key, 'ObservableValue': observable_value,\n",
    "    })\n",
    "    return test_row\n",
    "\n",
    "if USE_TEST_DATA_LOGS and combined_logs_df is not None:\n",
    "    enabled_log_sources = [s for s, c in LOG_SOURCES.items() if c['enabled']]\n",
    "    test_observables = [(\"ipv4-addr:value\", TEST_IP), (\"domain-name:value\", TEST_DOMAIN), (\"url:value\", TEST_URL), (\"file:hashes.'SHA-256'\", TEST_FILE_HASH)]\n",
    "    rows = []\n",
    "    if enabled_log_sources:\n",
    "        for s, cfg in LOG_SOURCES.items():\n",
    "            if cfg['enabled']:\n",
    "                for k, v in test_observables: rows.append(create_test_log_row(s, k, v))\n",
    "    else:\n",
    "        for k, v in test_observables: rows.append(create_test_log_row(\"TestLogSource\", k, v))\n",
    "    if rows:\n",
    "        test_df = spark.createDataFrame(rows, schema=combined_logs_df.schema)\n",
    "        combined_logs_df = combined_logs_df.union(test_df).cache()\n",
    "        print(f\"✓ Injected {len(rows)} test log entries\")\n",
    "\n",
    "if USE_TEST_DATA_THREAT_INTEL:\n",
    "    test_obs = [(\"ipv4-addr:value\", TEST_IP), (\"domain-name:value\", TEST_DOMAIN), (\"url:value\", TEST_URL), (\"file:hashes.'SHA-256'\", TEST_FILE_HASH)]\n",
    "    for k, v in test_obs:\n",
    "        if k in indicator_dfs and indicator_dfs[k] is not None:\n",
    "            ti_row = create_test_ti_row(k, v)\n",
    "            if ti_row:\n",
    "                indicator_dfs[k] = indicator_dfs[k].union(spark.createDataFrame([ti_row], schema=indicator_dfs[k].schema))\n",
    "    print(\"✓ Injected test TI indicators where possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf9647",
   "metadata": {},
   "source": [
    "## Filter to intersection and join (strict/current/loose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ffae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine indicators\n",
    "deduped_indicators_df = None\n",
    "for key, df in indicator_dfs.items():\n",
    "    if df is not None:\n",
    "        sel = df.select('ObservableValue','ObservableKey','ThreatActors','Id','Pattern','TI_TenantId','TI_TimeGenerated','ValidFrom','ValidUntil','Data', 'TTPs')\n",
    "        deduped_indicators_df = sel if deduped_indicators_df is None else deduped_indicators_df.union(sel)\n",
    "        print(f\"✓ Added {key} indicators: {sel.count():,} records\")\n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            sel.show(5, truncate=True)\n",
    "\n",
    "if deduped_indicators_df is None:\n",
    "    raise RuntimeError('No threat intelligence indicators available after filtering')\n",
    "else:\n",
    "    if SHOW_DEBUG_LOGS or REDUCED_DEBUG_LOGS:\n",
    "        indicators_count = deduped_indicators_df.count()\n",
    "        print(f\"✓ Combined indicators: {indicators_count}\")\n",
    "\n",
    "# Check for MatchIds with multiple ObservableValues\n",
    "ti_check = deduped_indicators_df.groupBy(\"Id\").agg(\n",
    "    collect_set(\"ObservableValue\").alias(\"ObservableValues\")\n",
    ").filter(size(\"ObservableValues\") > 1)\n",
    "\n",
    "if ti_check.count() > 0:\n",
    "    print(\"WARNING: Found TI indicators with multiple observable values:\")\n",
    "    ti_check.show(truncate=False)\n",
    "\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    print(\"Sample of combined logs DataFrame:\")\n",
    "    combined_logs_df.show()\n",
    "\n",
    "# check for combined logs with multiple observable values\n",
    "combined_log_check = combined_logs_df.groupBy(\"ObservableValue\").agg(\n",
    "    collect_set(\"ObservableValue\").alias(\"ObservableValues\")\n",
    ").filter(size(\"ObservableValues\") > 1)\n",
    "\n",
    "if combined_log_check.count() > 0:\n",
    "    print(\"WARNING: Found combined logs with multiple observable values:\")\n",
    "    combined_log_check.show(truncate=False)\n",
    "\n",
    "distinct_log_values = combined_logs_df.select('ObservableValue').distinct()\n",
    "print(\"Distinct log values:\")\n",
    "if SHOW_DEBUG_LOGS or REDUCED_DEBUG_LOGS:\n",
    "    distinct_log_count = distinct_log_values.count()\n",
    "    print(f\"✓ Found {distinct_log_count:,} distinct observable values in logs\")\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    distinct_log_values.show(truncate=False)\n",
    "\n",
    "distinct_indicator_values = deduped_indicators_df.select('ObservableValue').distinct()\n",
    "print(\"Distinct indicator values:\")\n",
    "if SHOW_DEBUG_LOGS or REDUCED_DEBUG_LOGS:\n",
    "    distinct_indicator_count = distinct_indicator_values.count()\n",
    "    print(f\"✓ Found {distinct_indicator_count:,} distinct observable values in indicators\")\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    distinct_indicator_values.show(truncate=False)\n",
    "\n",
    "intersection_values = distinct_log_values.join(broadcast(distinct_indicator_values), ['ObservableValue'], 'inner').repartition('ObservableValue').cache()\n",
    "print(f\"✓ Found {intersection_values.count():,} intersecting observable values between logs and indicators\")\n",
    "\n",
    "filtered_logs_df = combined_logs_df.join(broadcast(intersection_values), ['ObservableValue'], 'inner').cache()\n",
    "filtered_ti_df = deduped_indicators_df.join(broadcast(intersection_values), ['ObservableValue'], 'inner').cache()\n",
    "\n",
    "# Optional auto-tune before the heavy join\n",
    "maybe_auto_tune_shuffle([filtered_logs_df, filtered_ti_df])\n",
    "\n",
    "logs_alias = filtered_logs_df.select(\n",
    "    col('ObservableValue'),\n",
    "    col('LogSource').alias('logs_LogSource'),\n",
    "    col('IdField').alias('logs_IdField'),\n",
    "    col('Id').cast('string').alias('logs_Id'),\n",
    "    col('TenantId').alias('logs_TenantId'),\n",
    "    col('TimeGenerated').alias('logs_TimeGenerated')\n",
    ")\n",
    " \n",
    "ti_alias = filtered_ti_df.select(\n",
    "    col('ObservableValue'),\n",
    "    col('ObservableKey').alias('ti_ObservableKey'),\n",
    "    col('ThreatActors').alias('ti_ThreatActors'),\n",
    "    col('TTPs').alias('ti_TTPs'),\n",
    "    col('Id').alias('ti_Id'),\n",
    "    col('Pattern').alias('ti_Pattern'),\n",
    "    col('TI_TenantId').alias('ti_TI_TenantId'),\n",
    "    col('TI_TimeGenerated').alias('ti_TI_TimeGenerated'),\n",
    "    col('ValidFrom').alias('ti_ValidFrom'),\n",
    "    col('ValidUntil').alias('ti_ValidUntil'),\n",
    "    col('Data').alias('ti_Data')\n",
    ")\n",
    "\n",
    "if MATCH_MODE == 'strict':\n",
    "    cond = (\n",
    "        logs_alias[\"ObservableValue\"] == ti_alias[\"ObservableValue\"]\n",
    "    ) & (\n",
    "        logs_alias[\"logs_TimeGenerated\"] >= ti_alias[\"ti_ValidFrom\"]\n",
    "    ) & (\n",
    "        logs_alias[\"logs_TimeGenerated\"] <= ti_alias[\"ti_ValidUntil\"]\n",
    "    )\n",
    "    base_join = logs_alias.join(broadcast(ti_alias), cond, 'inner')\n",
    "else:\n",
    "    base_join = logs_alias.join(broadcast(ti_alias), ['ObservableValue'], 'inner')\n",
    "\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    print(\"Sample of logs alias:\")\n",
    "    logs_alias.show(truncate=False)\n",
    "    print(\"Sample of ti alias:\")\n",
    "    ti_alias.show(truncate=True)\n",
    "\n",
    "# Select and alias columns to avoid ambiguity downstream\n",
    "matched_indicators_df = base_join.select(\n",
    "    'logs_LogSource',\n",
    "    'logs_IdField',\n",
    "    'logs_Id',\n",
    "    'logs_TenantId',\n",
    "    'logs_TimeGenerated',\n",
    "    'ObservableValue',\n",
    "    'ti_Id',\n",
    "    'ti_ObservableKey',\n",
    "    'ti_Pattern',\n",
    "    'ti_TI_TenantId',\n",
    "    'ti_TI_TimeGenerated',\n",
    "    'ti_ValidFrom',\n",
    "    'ti_ValidUntil',\n",
    "    'ti_TTPs',\n",
    "    'ti_ThreatActors',\n",
    "    'ti_Data'\n",
    ")\n",
    "\n",
    "if SHOW_DEBUG_LOGS or REDUCED_DEBUG_LOGS:\n",
    "    print(f\"✓ Matched indicators count: {matched_indicators_df.count():,}\")\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    matched_indicators_df.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f5d0a",
   "metadata": {},
   "source": [
    "## Build results and aggregate (base schema retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06491ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build detailed matches\n",
    "job_id = str(uuid.uuid4())\n",
    "data_schema = StructType(\n",
    "    [\n",
    "        StructField(\"indicator_types\", ArrayType(StringType()), True),\n",
    "        StructField(\"threat_actors\", ArrayType(StringType()), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "result_df = (\n",
    "    matched_indicators_df.withColumn(\"MatchId\", col(\"ti_Id\"))\n",
    "    .withColumn(\"JobId\", lit(job_id))\n",
    "    .withColumn(\"JobStartTime\", job_start_time)\n",
    "    .withColumn(\"JobEndTime\", current_timestamp())\n",
    "    .withColumn(\"MatchType\", lit(\"IoC\"))\n",
    "    .withColumn(\"TIReferenceId\", col(\"ti_Id\"))\n",
    "    .withColumn(\"TIValue\", col(\"ti_Pattern\"))\n",
    "    .withColumn(\"MatchCount\", lit(1).cast(\"Long\"))\n",
    "    .withColumn(\n",
    "        \"EventReferences\",\n",
    "        array(\n",
    "            struct(\n",
    "                col(\"logs_LogSource\").alias(\"Table\"),\n",
    "                col(\"logs_IdField\").alias(\"RecordIdField\"),\n",
    "                col(\"logs_Id\").alias(\"RecordId\"),\n",
    "                col(\"logs_TimeGenerated\").alias(\"TimeGenerated\"),\n",
    "                col(\"ti_ObservableKey\").alias(\"LogField\"),\n",
    "                col(\"ObservableValue\").alias(\"MatchedValue\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\"Data_parsed\", from_json(col(\"ti_Data\"), data_schema))\n",
    "    .withColumn(\"TTPs\", coalesce(col(\"ti_TTPs\"), col(\"Data_parsed.indicator_types\")))\n",
    "    .withColumn(\n",
    "        \"ThreatActors\",\n",
    "        coalesce(\n",
    "            col(\"ti_ThreatActors\"), \n",
    "            col(\"Data_parsed.threat_actors\"),\n",
    "            array(lit(\"Unknown Actor\"))\n",
    "        ),\n",
    "    )\n",
    "    .withColumn(\"EnrichmentContext\", col(\"ti_Data\"))\n",
    "    .withColumn(\"TenantId\", col(\"logs_TenantId\"))\n",
    "    .withColumn(\"TimeGenerated\", col(\"logs_TimeGenerated\"))\n",
    "    .withColumn(\"TI_TenantId\", col(\"ti_TI_TenantId\"))\n",
    "    .withColumn(\"TI_TimeGenerated\", col(\"ti_TI_TimeGenerated\"))\n",
    "    .drop(\"Data_parsed\")\n",
    ")\n",
    "\n",
    "result_df = result_df.select(\n",
    "    \"MatchId\",\n",
    "    \"JobId\",\n",
    "    \"JobStartTime\",\n",
    "    \"JobEndTime\",\n",
    "    \"MatchType\",\n",
    "    col(\"ti_ObservableKey\").alias(\"ObservableType\"),\n",
    "    col(\"ObservableValue\").alias(\"ObservableValue\"),\n",
    "    \"TIReferenceId\",\n",
    "    \"TIValue\",\n",
    "    \"MatchCount\",\n",
    "    \"EventReferences\",\n",
    "    \"TTPs\",\n",
    "    \"ThreatActors\",\n",
    "    \"EnrichmentContext\",\n",
    "    \"TenantId\",\n",
    "    \"TimeGenerated\",\n",
    "    \"TI_TenantId\",\n",
    "    \"TI_TimeGenerated\",\n",
    ")\n",
    "\n",
    "if SHOW_DEBUG_LOGS:\n",
    "    rc = result_df.count()\n",
    "    print(f\"✓ Built results DataFrame with {rc:,} individual matches\")\n",
    "\n",
    "rolled_up_df = result_df.groupBy(\"MatchId\", \"ObservableValue\").agg(\n",
    "    first(\"JobId\").alias(\"JobId\"),\n",
    "    first(\"JobStartTime\").alias(\"JobStartTime\"),\n",
    "    first(\"JobEndTime\").alias(\"JobEndTime\"),\n",
    "    first(\"MatchType\").alias(\"MatchType\"),\n",
    "    first(\"ObservableType\").alias(\"ObservableType\"),\n",
    "    first(\"TIReferenceId\").alias(\"TIReferenceId\"),\n",
    "    first(\"TIValue\").alias(\"TIValue\"),\n",
    "    collect_list(\"EventReferences\").alias(\"_temp_EventReferences\"),\n",
    "    first(\"TTPs\").alias(\"TTPs\"),\n",
    "    first(\"ThreatActors\").alias(\"ThreatActors\"),\n",
    "    first(\"EnrichmentContext\").alias(\"EnrichmentContext\"),\n",
    "    first(\"TenantId\").alias(\"TenantId\"),\n",
    "    first(\"TimeGenerated\").alias(\"TimeGenerated\"),\n",
    "    first(\"TI_TenantId\").alias(\"TI_TenantId\"),\n",
    "    first(\"TI_TimeGenerated\").alias(\"TI_TimeGenerated\"),\n",
    ").withColumn(\n",
    "    \"_flattened_EventReferences\", \n",
    "    flatten(col(\"_temp_EventReferences\"))\n",
    ").withColumn(\n",
    "    \"MatchCount\",\n",
    "    size(col(\"_flattened_EventReferences\")).cast(\"Long\")\n",
    ").withColumn(\n",
    "    \"EventReferences\",\n",
    "    to_json(col(\"_flattened_EventReferences\"))\n",
    ").drop(\"_temp_EventReferences\", \"_flattened_EventReferences\")\n",
    "\n",
    "# Also serialize TTPs and ThreatActors to JSON strings for consistent storage\n",
    "rolled_up_df = rolled_up_df.withColumn(\n",
    "    \"TTPs\", \n",
    "    when(col(\"TTPs\").isNotNull(), to_json(col(\"TTPs\"))).otherwise(lit(None))\n",
    ").withColumn(\n",
    "    \"ThreatActors\",\n",
    "    when(col(\"ThreatActors\").isNotNull(), to_json(col(\"ThreatActors\"))).otherwise(lit(None))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820477a",
   "metadata": {},
   "source": [
    "## Save results (incremental by MatchId)\n",
    "\n",
    "Note: We keep the base table schema (EventReferences as array<struct>) and append only new MatchIds. Event-level dedupe can be added later with a merge routine if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bad823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: clear existing\n",
    "CLEAR_RESULTS_TABLE = False\n",
    "if CLEAR_RESULTS_TABLE:\n",
    "    try:\n",
    "        data_provider.delete_table(RESULTS_TABLE, WORKSPACE_NAME)\n",
    "        print(f\"✓ Deleted existing results table: {RESULTS_TABLE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not delete table {RESULTS_TABLE}: {e}\")\n",
    "\n",
    "try:\n",
    "    # Try to read the existing table directly\n",
    "    existing_df = None\n",
    "    existing_count = 0\n",
    "\n",
    "    try:\n",
    "        existing_df = data_provider.read_table(RESULTS_TABLE, WORKSPACE_NAME)\n",
    "        existing_count = existing_df.count()\n",
    "        table_exists = existing_count > 0\n",
    "    except Exception as read_error:\n",
    "        # Table doesn't exist or can't be read\n",
    "        table_exists = False\n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            print(\n",
    "                f\"ℹ️ Table {RESULTS_TABLE} not found or empty: {str(read_error)[:100]}...\"\n",
    "            )\n",
    "\n",
    "    # Table doesn't exist or is empty - create new - exit early\n",
    "    if not table_exists:\n",
    "        print(f\"📁 Creating new results table: {RESULTS_TABLE}\")\n",
    "\n",
    "        # Show what we're creating\n",
    "        event_count = rolled_up_df.count()\n",
    "        initial_events = rolled_up_df.agg({\"MatchCount\": \"sum\"}).collect()[0][0] or 0\n",
    "\n",
    "        print(f\"  • Creating with {event_count:,} TI indicator records\")\n",
    "        print(f\"  • Total event references: {initial_events:,}\")\n",
    "\n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            print(\"\\n🔍 Sample of data being saved (first 20 rows):\")\n",
    "            rolled_up_df.show(20, truncate=True)\n",
    "\n",
    "        data_provider.save_as_table(rolled_up_df, RESULTS_TABLE, WORKSPACE_NAME)\n",
    "        print(\"✓ Created table with initial results\")\n",
    "    else:\n",
    "        print(f\"📁 Found existing results table: {RESULTS_TABLE}\")\n",
    "        print(f\"\\n📊 Initial counts:\")\n",
    "        print(f\"  • Existing records in table: {existing_count:,}\")\n",
    "\n",
    "        event_count = rolled_up_df.count()\n",
    "        new_count = rolled_up_df.count()\n",
    "        print(f\"  • New records to process: {new_count:,}\")\n",
    "\n",
    "        # Parse the existing EventReferences JSON back to arrays and explode\n",
    "        existing_exploded = (\n",
    "            existing_df.withColumn(\n",
    "                \"EventReferences_parsed\",\n",
    "                from_json(\n",
    "                    col(\"EventReferences\"),\n",
    "                    ArrayType(\n",
    "                        StructType(\n",
    "                            [\n",
    "                                StructField(\"Table\", StringType(), True),\n",
    "                                StructField(\"RecordId\", StringType(), True),\n",
    "                                StructField(\"TimeGenerated\", TimestampType(), True),\n",
    "                                StructField(\"LogField\", StringType(), True),\n",
    "                                StructField(\"MatchedValue\", StringType(), True),\n",
    "                            ]\n",
    "                        )\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "            .select(\"MatchId\", explode(col(\"EventReferences_parsed\")).alias(\"EventRef\"))\n",
    "            .select(\n",
    "                col(\"MatchId\").alias(\"existing_MatchId\"),\n",
    "                col(\"EventRef.Table\").alias(\"existing_Table\"),\n",
    "                col(\"EventRef.RecordId\").alias(\"existing_RecordId\"),\n",
    "                col(\"EventRef.TimeGenerated\").alias(\"existing_TimeGenerated\"),\n",
    "                col(\"EventRef.LogField\").alias(\"existing_LogField\"),\n",
    "                col(\"EventRef.MatchedValue\").alias(\"existing_MatchedValue\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        existing_event_count = existing_exploded.count()\n",
    "        print(f\"  • Existing individual events: {existing_event_count:,}\")\n",
    "\n",
    "        # Parse and explode the new results\n",
    "        new_exploded = (\n",
    "            rolled_up_df.withColumn(\n",
    "                \"EventReferences_parsed\",\n",
    "                from_json(\n",
    "                    col(\"EventReferences\"),\n",
    "                    ArrayType(\n",
    "                        StructType(\n",
    "                            [\n",
    "                                StructField(\"Table\", StringType(), True),\n",
    "                                StructField(\"RecordId\", StringType(), True),\n",
    "                                StructField(\"TimeGenerated\", TimestampType(), True),\n",
    "                                StructField(\"LogField\", StringType(), True),\n",
    "                                StructField(\"MatchedValue\", StringType(), True),\n",
    "                            ]\n",
    "                        )\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "            .select(\"*\", explode(col(\"EventReferences_parsed\")).alias(\"EventRef\"))\n",
    "            .select(\n",
    "                \"*\",\n",
    "                col(\"EventRef.Table\").alias(\"new_Table\"),\n",
    "                col(\"EventRef.RecordId\").alias(\"new_RecordId\"),\n",
    "                col(\"EventRef.TimeGenerated\").alias(\"new_TimeGenerated\"),\n",
    "                col(\"EventRef.LogField\").alias(\"new_LogField\"),\n",
    "                col(\"EventRef.MatchedValue\").alias(\"new_MatchedValue\"),\n",
    "            )\n",
    "            .drop(\"EventRef\", \"EventReferences_parsed\")\n",
    "        )\n",
    "\n",
    "        new_event_count = new_exploded.count()\n",
    "        print(f\"  • New individual events to check: {new_event_count:,}\")\n",
    "\n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            print(\"\\n🔍 Analyzing RecordId patterns:\")\n",
    "            \n",
    "            # Check how many records have null RecordIds\n",
    "            new_null_ids = new_exploded.filter(col(\"new_RecordId\").isNull()).count()\n",
    "            new_total = new_exploded.count()\n",
    "            print(f\"  • New events with NULL RecordId: {new_null_ids:,} / {new_total:,} ({(new_null_ids/new_total*100):.1f}%)\" if new_total > 0 else \"  • No new events\")\n",
    "            \n",
    "            existing_null_ids = existing_exploded.filter(col(\"existing_RecordId\").isNull()).count()\n",
    "            existing_total = existing_exploded.count()\n",
    "            print(f\"  • Existing events with NULL RecordId: {existing_null_ids:,} / {existing_total:,} ({(existing_null_ids/existing_total*100):.1f}%)\" if existing_total > 0 else \"  • No existing events\")\n",
    "            \n",
    "            # Show sample of events with NULL RecordIds\n",
    "            if new_null_ids > 0:\n",
    "                print(\"\\n  Sample of new events with NULL RecordId:\")\n",
    "                new_exploded.filter(col(\"new_RecordId\").isNull()).select(\n",
    "                    \"MatchId\", \"new_Table\", \"new_RecordId\", \"new_TimeGenerated\", \"new_LogField\", \"new_MatchedValue\"\n",
    "                ).show(5, truncate=False)\n",
    "            \n",
    "            # Check for exact duplicates (should find them if dedup is working)\n",
    "            print(\"\\n🔍 Checking for exact matches between new and existing:\")\n",
    "            exact_matches = new_exploded.join(\n",
    "                existing_exploded,\n",
    "                (new_exploded.MatchId == existing_exploded.existing_MatchId)\n",
    "                & (new_exploded.new_Table == existing_exploded.existing_Table)\n",
    "                & (\n",
    "                    ((new_exploded.new_RecordId.isNotNull()) & (new_exploded.new_RecordId == existing_exploded.existing_RecordId))\n",
    "                    | ((new_exploded.new_RecordId.isNull()) & (existing_exploded.existing_RecordId.isNull()))\n",
    "                )\n",
    "                & (new_exploded.new_TimeGenerated == existing_exploded.existing_TimeGenerated)\n",
    "                & (new_exploded.new_LogField == existing_exploded.existing_LogField),\n",
    "                \"inner\"\n",
    "            )\n",
    "            \n",
    "            exact_count = exact_matches.count()\n",
    "            print(f\"  • Found {exact_count:,} exact matches (these should be deduplicated)\")\n",
    "            \n",
    "            if exact_count > 0:\n",
    "                print(\"\\n  Sample of matches that should be deduplicated:\")\n",
    "                exact_matches.select(\n",
    "                    \"MatchId\", \"new_Table\", \"new_RecordId\", \"new_MatchedValue\", \"existing_MatchedValue\"\n",
    "                ).show(10, truncate=False)\n",
    "\n",
    "        # Show sample of what we're comparing (for debugging)\n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            print(\"\\n🔍 Sample of existing events (first 5):\")\n",
    "            existing_exploded.show(5, truncate=False)\n",
    "\n",
    "            print(\"\\n🔍 Sample of new events (first 5):\")\n",
    "            new_exploded.select(\n",
    "                \"MatchId\",\n",
    "                \"new_Table\",\n",
    "                \"new_RecordId\",\n",
    "                \"new_TimeGenerated\",\n",
    "                \"new_LogField\",\n",
    "                \"new_MatchedValue\",\n",
    "            ).show(5, truncate=False)\n",
    "\n",
    "        # Anti-join to find truly new event references\n",
    "        new_events_only = new_exploded.join(\n",
    "            existing_exploded,\n",
    "            (new_exploded.MatchId == existing_exploded.existing_MatchId)\n",
    "            & (new_exploded.new_Table == existing_exploded.existing_Table)\n",
    "            & (\n",
    "                # If RecordId exists, use it for matching\n",
    "                (\n",
    "                    (new_exploded.new_RecordId.isNotNull())\n",
    "                    & (existing_exploded.existing_RecordId.isNotNull())\n",
    "                    & (new_exploded.new_RecordId == existing_exploded.existing_RecordId)\n",
    "                )\n",
    "                |\n",
    "                # Otherwise use combination of fields for matching\n",
    "                # This handles the case where RecordId is null\n",
    "                (\n",
    "                    (new_exploded.new_RecordId.isNull())\n",
    "                    & (existing_exploded.existing_RecordId.isNull())\n",
    "                    & (\n",
    "                        new_exploded.new_TimeGenerated\n",
    "                        == existing_exploded.existing_TimeGenerated\n",
    "                    )\n",
    "                    & (new_exploded.new_LogField == existing_exploded.existing_LogField)\n",
    "                    & (\n",
    "                        # Compare normalized values to handle case differences\n",
    "                        lower(trim(new_exploded.new_MatchedValue))\n",
    "                        == lower(trim(existing_exploded.existing_MatchedValue))\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            \"leftanti\",\n",
    "        )\n",
    "        \n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            # Check for mismatched observable values\n",
    "            mismatch_check = new_events_only.filter(\n",
    "                col(\"ObservableValue\") != col(\"new_MatchedValue\")\n",
    "            ).select(\n",
    "                \"MatchId\",\n",
    "                \"ObservableValue\",\n",
    "                \"new_MatchedValue\",\n",
    "                \"new_LogField\",\n",
    "                \"new_Table\"\n",
    "            ).limit(10)\n",
    "            \n",
    "            mismatch_count = mismatch_check.count()\n",
    "            if mismatch_count > 0:\n",
    "                print(f\"\\n⚠️ WARNING: Found events where ObservableValue != MatchedValue:\")\n",
    "                mismatch_check.show(truncate=False)\n",
    "                print(f\"Total mismatches: {mismatch_count}\")\n",
    "\n",
    "        actual_new_events = new_events_only.count()\n",
    "        duplicate_events = new_event_count - actual_new_events\n",
    "\n",
    "        print(f\"\\n📈 Deduplication results:\")\n",
    "        print(f\"  • Total events checked: {new_event_count:,}\")\n",
    "        print(\n",
    "            f\"  • Duplicate events (already exist): {duplicate_events:,} ({(duplicate_events/new_event_count*100):.1f}%)\"\n",
    "            if new_event_count > 0\n",
    "            else \"  • Duplicate events: 0\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  • Truly new events to add: {actual_new_events:,} ({(actual_new_events/new_event_count*100):.1f}%)\"\n",
    "            if new_event_count > 0\n",
    "            else \"  • Truly new events: 0\"\n",
    "        )\n",
    "\n",
    "        # Show which MatchIds have new events\n",
    "        if SHOW_DEBUG_LOGS:\n",
    "            matchids_with_new = new_events_only.select(\"MatchId\").distinct()\n",
    "            print(f\"\\n🎯 MatchIds with new events: {matchids_with_new.count()}\")\n",
    "\n",
    "            # Show breakdown by Table\n",
    "            print(\"\\n📊 New events by table:\")\n",
    "            new_events_only.groupBy(\"new_Table\").count().orderBy(\n",
    "                \"count\", ascending=False\n",
    "            ).show()\n",
    "\n",
    "        if SHOW_DEBUG_LOGS and actual_new_events > 0:\n",
    "            print(\"\\n🔍 Sample of TRULY NEW events being added (first 20):\")\n",
    "            new_events_only.select(\n",
    "                \"MatchId\",\n",
    "                \"ObservableValue\",\n",
    "                \"new_Table\",\n",
    "                \"new_RecordId\",\n",
    "                \"new_TimeGenerated\",\n",
    "                \"new_LogField\",\n",
    "                \"new_MatchedValue\",\n",
    "            ).show(20, truncate=False)\n",
    "\n",
    "        # Group by observable value to see what's triggering new matches\n",
    "        print(\"\\n📊 New events by ObservableValue (top 10):\")\n",
    "        new_events_only.groupBy(\"ObservableValue\").agg(\n",
    "            spark_count(\"*\").alias(\"NewEventCount\"),\n",
    "            collect_set(\"new_LogField\").alias(\"LogFields\"),\n",
    "            collect_set(\"new_Table\").alias(\"Tables\"),\n",
    "        ).orderBy(\"NewEventCount\", ascending=False).show(10, truncate=False)\n",
    "\n",
    "        # Show a few specific examples with full details\n",
    "        print(\"\\n🔬 Detailed examples of new events (first 5 unique MatchIds):\")\n",
    "        sample_matchids = new_events_only.select(\"MatchId\").distinct().limit(5)\n",
    "        for row in sample_matchids.collect():\n",
    "            match_id = row[\"MatchId\"]\n",
    "            print(f\"\\n  MatchId: {match_id[:50]}...\")\n",
    "            sample_events = (\n",
    "                new_events_only.filter(col(\"MatchId\") == match_id)\n",
    "                .select(\n",
    "                    \"ObservableValue\",\n",
    "                    \"new_Table\",\n",
    "                    \"new_LogField\",\n",
    "                    \"new_MatchedValue\",\n",
    "                    \"new_TimeGenerated\",\n",
    "                )\n",
    "                .limit(3)\n",
    "            )\n",
    "            sample_events.show(truncate=False)\n",
    "\n",
    "            # Re-aggregate the new events back by MatchId\n",
    "            new_matches_df = (\n",
    "                new_events_only.groupBy(\"MatchId\", \"ObservableValue\")\n",
    "                .agg(\n",
    "                    first(\"JobId\").alias(\"JobId\"),\n",
    "                    first(\"JobStartTime\").alias(\"JobStartTime\"),\n",
    "                    first(\"JobEndTime\").alias(\"JobEndTime\"),\n",
    "                    first(\"MatchType\").alias(\"MatchType\"),\n",
    "                    first(\"ObservableType\").alias(\"ObservableType\"),\n",
    "                    first(\"TIReferenceId\").alias(\"TIReferenceId\"),\n",
    "                    first(\"TIValue\").alias(\"TIValue\"),\n",
    "                    collect_list(\n",
    "                        struct(\n",
    "                            col(\"new_Table\").alias(\"Table\"),\n",
    "                            col(\"new_RecordId\").alias(\"RecordId\"),\n",
    "                            col(\"new_TimeGenerated\").alias(\"TimeGenerated\"),\n",
    "                            col(\"new_LogField\").alias(\"LogField\"),\n",
    "                            col(\"new_MatchedValue\").alias(\"MatchedValue\"),\n",
    "                        )\n",
    "                    ).alias(\"_new_EventReferences\"),\n",
    "                    first(\"TTPs\").alias(\"TTPs\"),\n",
    "                    first(\"ThreatActors\").alias(\"ThreatActors\"),\n",
    "                    first(\"EnrichmentContext\").alias(\"EnrichmentContext\"),\n",
    "                    first(\"TenantId\").alias(\"TenantId\"),\n",
    "                    current_timestamp().alias(\n",
    "                        \"TimeGenerated\"\n",
    "                    ),\n",
    "                    first(\"TI_TenantId\").alias(\"TI_TenantId\"),\n",
    "                    first(\"TI_TimeGenerated\").alias(\"TI_TimeGenerated\"),\n",
    "                )\n",
    "                .withColumn(\"MatchCount\", size(col(\"_new_EventReferences\")))\n",
    "                .withColumn(\"EventReferences\", to_json(col(\"_new_EventReferences\")))\n",
    "                .drop(\"_new_EventReferences\")\n",
    "            )\n",
    "\n",
    "            # Filter out any MatchIds that now have 0 events (all were duplicates)\n",
    "            new_matches_df = new_matches_df.filter(col(\"MatchCount\") > 0)\n",
    "\n",
    "            to_add = new_matches_df.count()\n",
    "            new_events_count = (\n",
    "                new_matches_df.agg({\"MatchCount\": \"sum\"}).collect()[0][0] or 0\n",
    "            )\n",
    "\n",
    "            print(f\"\\n✅ Final summary:\")\n",
    "            print(f\"  • New TI indicator records to add: {to_add:,}\")\n",
    "            print(f\"  • Total new event references in those records: {new_events_count:,}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error saving results: {e}\")\n",
    "    # PJS can NOT handle full tracebacks in logs at the moment.  Only use this locally until a broader fix is available.\n",
    "    # if SHOW_DEBUG_LOGS:\n",
    "    #     import traceback\n",
    "    #     traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204f95f",
   "metadata": {},
   "source": [
    "## Threat actor summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# THREAT ACTOR DETECTION SUMMARY\n",
    "# ===============================================================================\n",
    "if SHOW_STATS and rolled_up_df.count() > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"THREAT ACTOR DETECTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Parse ThreatActors JSON strings back to arrays for analysis\n",
    "    threat_actor_df = rolled_up_df.withColumn(\n",
    "        \"ThreatActorArray\",\n",
    "        from_json(col(\"ThreatActors\"), ArrayType(StringType()))\n",
    "    ).select(\n",
    "        \"MatchId\",\n",
    "        \"ObservableType\", \n",
    "        \"ObservableValue\",\n",
    "        \"MatchCount\",\n",
    "        explode(col(\"ThreatActorArray\")).alias(\"ThreatActor\")\n",
    "    )\n",
    "    \n",
    "    # Overall threat actor statistics\n",
    "    actor_stats = threat_actor_df.groupBy(\"ThreatActor\").agg(\n",
    "        spark_count(\"MatchId\").alias(\"UniqueIndicators\"),\n",
    "        sum(\"MatchCount\").alias(\"TotalEvents\"),\n",
    "        collect_set(\"ObservableType\").alias(\"IndicatorTypes\"),\n",
    "        count_distinct(\"ObservableValue\").alias(\"UniqueObservables\")\n",
    "    ).orderBy(col(\"TotalEvents\").desc())\n",
    "    \n",
    "    print(\"\\n📊 Threat Actors Detected (by event volume):\")\n",
    "    print(\"-\" * 60)\n",
    "    actor_stats.show(20, truncate=False)\n",
    "    \n",
    "    # Top indicators per threat actor\n",
    "    print(\"\\n🎯 Top Indicators by Threat Actor:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    top_actors = actor_stats.limit(5).collect()\n",
    "    for actor_row in top_actors:\n",
    "        actor_name = actor_row[\"ThreatActor\"]\n",
    "        if actor_name != \"Unknown Actor\":\n",
    "            print(f\"\\n  {actor_name}:\")\n",
    "            \n",
    "            actor_indicators = threat_actor_df.filter(\n",
    "                col(\"ThreatActor\") == actor_name\n",
    "            ).groupBy(\"ObservableValue\", \"ObservableType\").agg(\n",
    "                sum(\"MatchCount\").alias(\"Events\")\n",
    "            ).orderBy(col(\"Events\").desc()).limit(3)\n",
    "            \n",
    "            for ind_row in actor_indicators.collect():\n",
    "                print(f\"    • {ind_row['ObservableValue']} ({ind_row['ObservableType']}) - {ind_row['Events']} events\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📈 Summary Statistics:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_actors = actor_stats.filter(col(\"ThreatActor\") != \"Unknown Actor\").count()\n",
    "    unknown_count = actor_stats.filter(col(\"ThreatActor\") == \"Unknown Actor\").collect()\n",
    "    unknown_events = unknown_count[0][\"TotalEvents\"] if unknown_count else 0\n",
    "    \n",
    "    total_events = actor_stats.agg(sum(\"TotalEvents\")).collect()[0][0]\n",
    "    attributed_events = total_events - unknown_events if total_events else 0\n",
    "    \n",
    "    print(f\"  • Identified Threat Actors: {total_actors}\")\n",
    "    print(f\"  • Total Events with Attribution: {attributed_events:,} ({(attributed_events/total_events*100):.1f}%)\" if total_events > 0 else \"  • Total Events with Attribution: 0\")\n",
    "    print(f\"  • Events without Attribution: {unknown_events:,} ({(unknown_events/total_events*100):.1f}%)\" if total_events > 0 else \"  • Events without Attribution: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0217ba9d",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77659bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook version:\", VERSION)\n",
    "\n",
    "if SHOW_DEBUG_LOGS or REDUCED_DEBUG_LOGS:\n",
    "    end = time.time()\n",
    "    # Standard synapse compute cost for big data analytics\n",
    "    COMPUTE_COST_PER_VCORE_HOUR = 0.15\n",
    "\n",
    "    # Calculate time spent and vcore hours based on cluster\n",
    "    elapsed_seconds = end - start\n",
    "    elapsed_minutes = elapsed_seconds / 60\n",
    "    elapsed_hours = elapsed_minutes / 60\n",
    "    num_executors = int(spark.conf.get(\"spark.executor.instances\"))\n",
    "    num_drivers = 1\n",
    "    cores_per_executor = int(spark.conf.get(\"spark.executor.cores\"))\n",
    "    total_vcores = (num_executors + num_drivers) * cores_per_executor\n",
    "    vcore_hours = total_vcores * elapsed_hours\n",
    "    job_cost = vcore_hours * COMPUTE_COST_PER_VCORE_HOUR\n",
    "    mc = matched_indicators_df.count()\n",
    "\n",
    "    # Output summary\n",
    "    print(f\"Cluster configs:\")\n",
    "    print(f\"Executors: {num_executors}\")\n",
    "    print(f\"Drivers: {num_drivers}\")\n",
    "    print(f\"VCores per executor: {cores_per_executor}\")\n",
    "    print(f\"Total vcores: {total_vcores}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Combined logs count: {combined_logs_count:,}\")\n",
    "    print(f\"Indicators count: {indicators_count:,}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Distinct log count: {distinct_log_count:,}\")\n",
    "    print(f\"Distinct indicator count: {distinct_indicator_count:,}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Matched log count: {mc:,}\")\n",
    "    print(f\"Matched rolled up indicator count: {event_count:,}\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"Runtime: {elapsed_minutes:.2f} minutes\")\n",
    "    print(\"------------------------------------------\")\n",
    "    print(f\"VCore-hours used: {vcore_hours:.2f}\")\n",
    "    print(f\"Job cost: ${job_cost:.2f}\")\n",
    "\n",
    "    print(\"\\n\\n------------------------------------------\")\n",
    "    print(f\"Indicators partitions: {deduped_indicators_df.rdd.getNumPartitions()}\")\n",
    "    print(f\"Logs partitions: {combined_logs_df.rdd.getNumPartitions()}\")\n",
    "    print(f\"Distinct indicators partitions: {distinct_indicator_values.rdd.getNumPartitions()}\")\n",
    "    print(f\"Distinct logs partitions: {distinct_log_values.rdd.getNumPartitions()}\")\n",
    "    print(f\"Intersecting values partitions: {intersection_values.rdd.getNumPartitions()}\")\n",
    "    print(f\"Rolled up result df partitions: {rolled_up_df.rdd.getNumPartitions()}\")\n",
    "    print(\"------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "large pool (16 vCores) [MS-Retroactive-Hunting]",
   "language": "Python",
   "name": "MSGLarge"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
